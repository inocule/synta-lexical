# Synta Lexical Analyzer

> An experimental lexer for an AI-native programming language designed for humanâ€“AI collaboration, agentic task execution, and deterministic reasoning.

Synta emphasizes static typing, concurrency primitives, intent-level debugging, and syntax optimized for LLM interpretability. The lexer provides the foundational token stream for Synta's compiler, AI runtime, multi-agent scheduler, and tooling ecosystem.

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SYNTA LEXER SYSTEM                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚  Frontend (UI)   â”‚  HTTP     â”‚  Backend Server  â”‚                â”‚
â”‚  â”‚   React + TS     â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    Go Runtime    â”‚                â”‚
â”‚  â”‚  localhost:5173  â”‚  JSON     â”‚  localhost:8080  â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚           â”‚                               â”‚                         â”‚
â”‚           â”‚                               â–¼                         â”‚
â”‚           â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚           â”‚                    â”‚  Lexer Core        â”‚               â”‚
â”‚           â”‚                    â”‚  (lexer/lexer.go)  â”‚               â”‚
â”‚           â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚           â”‚                             â”‚                           â”‚
â”‚           â”‚                             â–¼                           â”‚
â”‚           â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚           â”‚                    â”‚  Token Generator   â”‚               â”‚
â”‚           â”‚                    â”‚  (token/token.go)  â”‚               â”‚
â”‚           â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚           â”‚                             â”‚                           â”‚
â”‚           â–¼                             â–¼                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚             TOKEN STREAM OUTPUT                 â”‚                â”‚
â”‚  â”‚  [KEYWORD, IDENT, OPERATOR, NUMBER, STRING...]  â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PARSER PIPELINE (Future):                                          â”‚
â”‚  Token Stream â†’ AST Builder â†’ Type Checker â†’ Agent Scheduler        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DATA FLOW:
  Source Code (.synta) 
      â†“
  Lexer Tokenization
      â†“
  Token Metadata (line, col, type)
      â†“
  Parser / AI Runtime
```

---

## âœ¨ Core Features

### Lexical Analysis
- **Tokens**: Identifiers, numbers (int/float), strings, operators, delimiters
- **Comments**: Single-line `!>` and multi-line `<! ... !>` (unified token capture)
- **Delimiters**: Statement-end token `;` for clear boundaries
- **Tracking**: Precise line/column position for every token
- **Newlines**: Optional newline tokens for structure-aware parsers

### AI-Native Syntax
- **Keywords**: `think`, `reason`, `ask`, `observe`, `intent`, `pipeline`, `agent`, `concurrent`, `sequential`
- **Decorators**: `@agent`, `@task`, `@model`, `@pipeline` for agent definitions
- **Invocation**: Arrow operator `->` for type-safe AI calls
- **Types**: `int`, `float`, `bool`, `char`, `str`, `object`, `list<T>`
- **Operators**: 
  - `:=` for binding (immutable declaration)
  - `=:` for assignment (mutable update)

---

## ğŸ“ Project Structure

```
synta-lexical/
â”œâ”€â”€ backend-server/
â”‚   â””â”€â”€ main.go              # Go HTTP server
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ Components/
â”‚   â”‚   â”‚   â”œâ”€â”€ EditorPane.tsx      # Code input (left)
â”‚   â”‚   â”‚   â””â”€â”€ OutputTable.tsx     # Token output (right)
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â”œâ”€â”€ main.tsx
â”‚   â”‚   â””â”€â”€ types.ts
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ vite.config.ts
â”œâ”€â”€ lexer/
â”‚   â”œâ”€â”€ lexer.go             # Core tokenizer
â”œâ”€â”€ token/
â”‚   â””â”€â”€ token.go             # Token types & keywords
â”œâ”€â”€ go.mod
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Prerequisites
```bash
go version    # Requires Go 1.21+
node --version # Requires Node.js 16+
```

### Installation & Running

**1. Start Backend Server**
```bash
cd synta-lexical/cmd/server
go run main.go
# Server running at http://localhost:8080
```

**2. Start Frontend (New Terminal)**
```bash
cd synta-lexical/frontend
npm install           # First time only
npm run dev
# UI available at http://localhost:5173
```

**3. Open Browser**
Navigate to `http://localhost:5173` and start tokenizing!

---

## ğŸ“ Language Syntax Examples

### Agent Definition
```synta
@agent AICoder {
    role: "GitHub-integrated coding assistant",
    tools: [github_mcp, slm_chatbot, pdf_scanner],
    model: "local/llama-3.1-8b.gguf",
    mode: "hybrid",
    sys_prompt: "Up-to-date assistant for repositories"
}
```

### Task Execution
```synta
@task {
    response:str =: AICoder -> "Fix syntax errors"
    print(response)
}
```

### Variables & Types
```synta
!> Binding (immutable)
bind x:int := 10;
const PI := 3.14;

!> Assignment (mutable)
x =: 20;
```

### Functions
```synta
fn calculate(a:int, b:int) -> int do {
    if a > b {
        return a + b
    } else {
        return a - b
    }
}
```

### Intent Blocks
```synta
intent {
    goal: "Analyze Q4 sales trends";
    context: "Processed dataset from pipeline";
    reason: "Preprocessing completed successfully";
}
```

### Concurrency
```synta
for task in tasks concurrent {
    process(task);
}

pipeline analysis_flow {
    start analyze_dataset;
    then generate_report concurrent;
}
```

### Model Fine-Tuning
```synta
model, tokenizer =: unsloth.FastLanguageModel.from_pretrained(
    model_name: "unsloth/Phi-4-mini-instruct",
    max_seq_length: 2048,
    dtype: None,
    load_in_4bit: True
);
```

---

## ğŸ¯ Language Design Principles

### 1. Static & Strong Typing
- Variables use `name:type` annotation
- Assignment `=:` vs binding `:=` distinction
- Compile-time type checking for AI invocations

### 2. Intent-Level Debugging
- First-class `intent` blocks for AI chain-of-thought
- Context window replenishment support
- Deterministic introspection for agentic runtimes

### 3. Concurrency as Core Syntax
- `concurrent`, `parallel`, `sequential` as reserved words
- Runtime scheduler integration
- DAG-based task execution graphs

### 4. AI-Friendly Parsing
- Low-ambiguity operators (`=:`, `:=`, `->`)
- Mandatory braces for blocks
- Explicit type annotations
- Machine-readable pipeline syntax

### 5. Memory Safety
- Scoped lifetimes with `own` and `borrow` semantics
- Deterministic cleanup events
- Built-in garbage collection

### 6. Native LLM/SLM Integration
- Vector database identifiers
- RAG pipeline keywords
- RLHF-loop verbs
- Embedded model descriptors

### 7. Agentic Extensions
- Multi-agent orchestration keywords
- `depends_on`, `emit`, `listen` primitives
- Deterministic execution DAGs

---

## ğŸ”§ Extending the Lexer

### Add Keywords
```go
// In token/token.go
const (
    NEWKEYWORD = "NEWKEYWORD"
)

var Keywords = map[string]TokenType{
    "newkeyword": NEWKEYWORD,
}
```

### Add Operators
```go
// In lexer/lexer.go Tokenize() switch
case 'â˜…':
    tokens = append(tokens, Token{Type: STAR_OP, Literal: "â˜…"})
```

### Add Decorators
```go
// In lexer/lexer.go @ handling
case '@':
    if isLetter(peekChar()) {
        decorator := readDecorator()
        // Add new decorator logic
    }
```

---

## ğŸ“Š Token Types Reference

| Category | Examples |
|----------|----------|
| **Keywords** | `think`, `reason`, `intent`, `pipeline`, `concurrent` |
| **Decorators** | `@agent`, `@task`, `@model`, `@pipeline` |
| **Operators** | `:=` (bind), `=:` (assign), `->` (invoke) |
| **Types** | `int`, `float`, `bool`, `str`, `list<T>` |
| **Delimiters** | `~` (statement end), `{`, `}`, `(`, `)` |
| **Comments** | `!>` (single), `<! !>` (multi) |

---

## ğŸ§ª Testing

```bash
cd lexer
go test -v
```

---

## ğŸ“œ License

Experimental - Educational Use

---

## ğŸ¤ Contributing

This is an experimental language design. Contributions welcome for:
- Additional AI-native keywords
- Enhanced token metadata
- Performance optimizations
- Parser integration examples

---

**Built for the future of humanâ€“AI collaborative programming** ğŸš€